{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04cc4a8e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#### 方便探索使用\n",
    "import re\n",
    "from sklearn.model_selection import KFold\n",
    "from lightgbm import log_evaluation, early_stopping\n",
    "import lightgbm as lgb\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def del_PART_ORDER(PART_ORDER):\n",
    "    \"\"\"\n",
    "    读取处理数据\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    part_order = pd.read_csv(PART_ORDER).fillna(0)\n",
    "\n",
    "\n",
    "    part_order['MONTH'] = pd.to_datetime(part_order['MONTH']).dt.strftime('%Y-%m-%d')\n",
    "    part_order['MONTH_INT'] = part_order.apply(month_int_time, axis=1)\n",
    "    # 去重\n",
    "    part_order = part_order.drop_duplicates(subset=['PART_NO', 'MONTH_INT'], keep='last')\n",
    "    # 排序\n",
    "    part_order = part_order.sort_values(by='MONTH_INT')\n",
    "\n",
    "    # print(part_order)\n",
    "\n",
    "    return part_order\n",
    "\n",
    "\n",
    "def del_PART_LIST(PART_LIST):\n",
    "\n",
    "    part_list = pd.read_csv(PART_LIST, encoding=\"GBK\")\n",
    "    part_list['PIDT'] = pd.to_datetime(part_list['PIDT']).dt.strftime('%Y-%m-%d')\n",
    "    ONEHOT_COLUMNS = ['CAR_CLASS', 'REPAIR_TYPE', 'TYPE_CODE', 'CONSTRUCT_NAME']\n",
    "\n",
    "    DROP = ['KPDS', 'PIDT']\n",
    "    for column in ONEHOT_COLUMNS:\n",
    "        part_list = hot_coding_dispose(part_list, column)\n",
    "\n",
    "    part_list = part_list.drop(columns=DROP)\n",
    "    # 去重\n",
    "    part_list = part_list.drop_duplicates(['PART_NO'],keep='last')\n",
    "\n",
    "    return part_list\n",
    "\n",
    "def del_TESTDATA_ID(TESTDATA_ID):\n",
    "    part_predict_list = []\n",
    "    with open(TESTDATA_ID, 'r') as f:\n",
    "        for part in f.readlines():\n",
    "            part_predict_list.append(part[:-1])\n",
    "\n",
    "    return part_predict_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def month_int_time(a):\n",
    "    \"\"\"\n",
    "    从2015-1-1开始当前为第sum个月\n",
    "    :param a:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    month = int(a['MONTH'].split('-')[1]) - 1\n",
    "    year = (int(a['MONTH'].split('-')[0]) - 2015 ) * 12\n",
    "    sum = year + month\n",
    "    return sum\n",
    "\n",
    "# def hot_encode():\n",
    "\n",
    "\n",
    "def hot_coding_dispose(data, column):\n",
    "    \"\"\"\n",
    "    使用热编码完成字符串转化\n",
    "    :param data: 数据集\n",
    "    :param column: 需要编码和删除的列名\n",
    "    :return: 合并之后的集合\n",
    "    \"\"\"\n",
    "    # 使用热门编码转化字符串\n",
    "    code = pd.DataFrame({column: data[column]})\n",
    "    code_DataFrame = pd.get_dummies(code)\n",
    "    # merge，改用concat\n",
    "    data = pd.concat([data, code_DataFrame], axis=1)\n",
    "    # 删除列\n",
    "    data = data.drop(columns=column)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def del_data(part_order, part_list, part_predict_list):\n",
    "    \"\"\"\n",
    "    特征处理\n",
    "    :param part_order:\n",
    "    :param part_list:\n",
    "    :param part_predict_list:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    part_train_x={}\n",
    "    part_train_y1={}\n",
    "    part_train_y3={}\n",
    "    # 预测使用\n",
    "    part_predict_x_lst={}\n",
    "    part_len = len(part_predict_list)\n",
    "    FEATURES = ['DM01','REPAIR_CNT','REPAIR_AMOUNT','RUN_DIST','MONTH_INT']\n",
    "    break_time = 11\n",
    "\n",
    "    # 合并特征\n",
    "    part_order_list = pd.merge(part_order, part_list, how='left', left_on='PART_NO', right_on='PART_NO')\n",
    "    del_feature = ['PART_NO', 'MONTH']\n",
    "    features_all = [i for i in part_order_list.columns if i not in del_feature]\n",
    "\n",
    "    for j in range(part_len):\n",
    "        if (j % 100 == 0):\n",
    "            print('\\r', \"数据处理进度：{}%\".format(j*100/part_len), end=\"\", flush=True)\n",
    "        order_part = part_order_list[part_order_list['PART_NO'] == part_predict_list[j]]  # 依次取出零件历史销售数据\n",
    "        for i in range(break_time, 81, 10):  ### （23,33,43,53,63，73）\n",
    "            if i == break_time:\n",
    "                ### 取i-23<=X<=i的内容\n",
    "                train_x = order_part[(order_part['MONTH_INT'] <= i) & (order_part['MONTH_INT'] >= i - break_time)][FEATURES]\n",
    "                ### train_x变为numpy并转置，reshape(1,-1)转化为一行\n",
    "                train_x = train_x.to_numpy().T.reshape(1, -1)\n",
    "                ### 获取第i+1个订单值 （未来一个月）\n",
    "                train_y1 = order_part[(order_part['MONTH_INT'] == i + 1)]['DM01']\n",
    "                ### 获取i+1,i+2,i+3的订单值的和 （未来三个月）\n",
    "                train_y3 = np.array([order_part[(order_part['MONTH_INT'] == i + 1) | (order_part['MONTH_INT'] == i + 2) | (order_part['MONTH_INT'] == i + 3)]['DM01'].sum()])\n",
    "            else:\n",
    "                ### 取i-23<=X<=i的内容\n",
    "                train_x_tmp = order_part[(order_part['MONTH_INT'] <= i) & (order_part['MONTH_INT'] >= i - break_time)][FEATURES]\n",
    "                train_x_tmp = train_x_tmp.to_numpy().T.reshape(1, -1)\n",
    "                train_y1_tmp = order_part[(order_part['MONTH_INT'] == i + 1)]['DM01']\n",
    "                train_y3_tmp = np.array([order_part[((order_part['MONTH_INT'] == i + 1) | (order_part['MONTH_INT'] == i + 2) | (order_part['MONTH_INT'] == i + 3))]['DM01'].sum()])\n",
    "                # 将某个零件的数据合并在一起\n",
    "                train_x = np.append(train_x, train_x_tmp, axis=0)\n",
    "                train_y1 = np.append(train_y1, train_y1_tmp, axis=0)\n",
    "                train_y3 = np.append(train_y3, train_y3_tmp, axis=0)\n",
    "\n",
    "        # 将所有零件的数据合并在一起\n",
    "        part_train_x[part_predict_list[j]] = train_x\n",
    "        part_train_y1[part_predict_list[j]] = train_y1  ###未来一个月\n",
    "        part_train_y3[part_predict_list[j]] = train_y3  ###未来三个月\n",
    "\n",
    "        #    最后一个输出数据\n",
    "        i = 83\n",
    "        train_x_lst = order_part[(order_part['MONTH_INT'] <= i) & (order_part['MONTH_INT'] >= i - break_time)][FEATURES]\n",
    "        train_x_lst = train_x_lst.to_numpy().T.reshape(1, -1)\n",
    "        part_predict_x_lst[part_predict_list[j]] = train_x_lst\n",
    "\n",
    "    # 模型所用数据\n",
    "\n",
    "    for part in part_predict_list:\n",
    "        if part == part_predict_list[0]:\n",
    "            train_x = part_train_x[part]\n",
    "            train_y1 = part_train_y1[part]\n",
    "            train_y3 = part_train_y3[part]\n",
    "            predict_x_lst = part_predict_x_lst[part]\n",
    "        else:\n",
    "            train_x = np.concatenate((train_x, part_train_x[part]))\n",
    "            train_y1 = np.concatenate((train_y1, part_train_y1[part]))\n",
    "            train_y3 = np.concatenate((train_y3, part_train_y3[part]))\n",
    "            predict_x_lst = np.concatenate((predict_x_lst, part_predict_x_lst[part]))\n",
    "\n",
    "    return train_x, train_y1, train_y3, predict_x_lst, features_all\n",
    "\n",
    "def format_model_data(part_train_x, part_predict_x_lst):\n",
    "\n",
    "    return pd.DataFrame(part_train_x), pd.DataFrame(part_predict_x_lst)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 数据处理进度：97.64758100310696%%"
     ]
    }
   ],
   "source": [
    "PART_ORDER = \"resources\\PART_ORDER.csv\"\n",
    "PART_LIST = \"resources\\PART_LIST.csv\"\n",
    "TESTDATA_ID = \"resources/testdata_id.txt\"\n",
    "\n",
    "\n",
    "part_order = del_PART_ORDER(PART_ORDER)\n",
    "part_list = del_PART_LIST(PART_LIST)\n",
    "part_predict_list = del_TESTDATA_ID(TESTDATA_ID)\n",
    "\n",
    "part_train_x, part_train_y1, part_train_y3, part_predict_x_lst, features_all = del_data(part_order, part_list, part_predict_list)\n",
    "\n",
    "part_train_x, part_predict_x_lst = format_model_data(part_train_x, part_predict_x_lst)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def mape_evaluate(preds, train_data):\n",
    "    \"\"\"\n",
    "    评估方法\n",
    "    :param preds:\n",
    "    :param train_data:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    mape = 0.0\n",
    "    train_y = train_data.get_label()\n",
    "    for i in range(0, len(preds)):\n",
    "        tmp = abs(float(int(train_y[i]) - int(preds[i]))) / (max(float(train_y[i]), float(preds[i])) + 1)\n",
    "        mape += tmp\n",
    "    mape /= len(preds)\n",
    "    return \"mape\", 1-mape\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def train_model(train_x, train_y, learning_rate, num_boost_round, stopping_rounds, objective, boosting):\n",
    "\n",
    "    params = {'num_leaves': 60,\n",
    "              'min_data_in_leaf': 50,\n",
    "              'objective': objective,\n",
    "              'max_depth': -1,\n",
    "              'learning_rate': learning_rate,\n",
    "              \"min_sum_hessian_in_leaf\": 6,\n",
    "              \"boosting\": boosting,\n",
    "              \"feature_fraction\": 0.9,\n",
    "              \"bagging_freq\": 1,\n",
    "              \"bagging_fraction\": 0.7,\n",
    "              \"bagging_seed\": 11,\n",
    "              \"lambda_l1\": 0.1,\n",
    "              # 'lambda_l2': 0.001,\n",
    "              \"verbosity\": -1,\n",
    "              \"nthread\": 3,\n",
    "              'metric': None,\n",
    "              \"random_state\": 111,\n",
    "              }\n",
    "    folds = KFold(n_splits=5, shuffle=True, random_state=111)\n",
    "    score = 0.0\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_x)):\n",
    "        print(\"fold {}\".format(fold_ + 1))\n",
    "        trn_data = lgb.Dataset(train_x.iloc[trn_idx], label=train_y[trn_idx])\n",
    "        val_data = lgb.Dataset(train_x.iloc[val_idx], label=train_y[val_idx])\n",
    "        clf = lgb.train(params,\n",
    "                trn_data,\n",
    "                num_boost_round=num_boost_round,\n",
    "                valid_sets=[trn_data, val_data],\n",
    "                valid_names=['train','train_test'],\n",
    "                callbacks=[log_evaluation(period=False),\n",
    "                           early_stopping(stopping_rounds=stopping_rounds)],\n",
    "                feval=mape_evaluate\n",
    "                )\n",
    "\n",
    "        score += clf.best_score['train_test']['mape']\n",
    "    return score / folds.n_splits\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1\n",
      "[0]\ttrain-rmse:761.30919\ttrain-mape:0.27051\tvalid-rmse:649.42408\tvalid-mape:0.25584\n",
      "[200]\ttrain-rmse:760.19244\ttrain-mape:0.45189\tvalid-rmse:648.15395\tvalid-mape:0.43773\n",
      "[400]\ttrain-rmse:759.12375\ttrain-mape:0.55770\tvalid-rmse:646.94110\tvalid-mape:0.54023\n",
      "[600]\ttrain-rmse:758.09618\ttrain-mape:0.62823\tvalid-rmse:645.77769\tvalid-mape:0.60804\n",
      "[800]\ttrain-rmse:757.10130\ttrain-mape:0.66784\tvalid-rmse:644.65401\tvalid-mape:0.64621\n",
      "[999]\ttrain-rmse:756.13872\ttrain-mape:0.69494\tvalid-rmse:643.56923\tvalid-mape:0.67102\n",
      "999\n",
      "fold 2\n",
      "[0]\ttrain-rmse:649.42408\ttrain-mape:0.25584\tvalid-rmse:761.30919\tvalid-mape:0.27051\n",
      "[200]\ttrain-rmse:648.15365\ttrain-mape:0.44144\tvalid-rmse:760.19280\tvalid-mape:0.45233\n",
      "[400]\ttrain-rmse:646.93933\ttrain-mape:0.55222\tvalid-rmse:759.12463\tvalid-mape:0.56168\n",
      "[600]\ttrain-rmse:645.77352\ttrain-mape:0.62515\tvalid-rmse:758.09934\tvalid-mape:0.62740\n",
      "[800]\ttrain-rmse:644.64726\ttrain-mape:0.66439\tvalid-rmse:757.10900\tvalid-mape:0.65834\n",
      "[999]\ttrain-rmse:643.55962\ttrain-mape:0.69262\tvalid-rmse:756.15145\tvalid-mape:0.67985\n",
      "999\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "train_x = part_train_x\n",
    "\n",
    "train_y1 = part_train_y1\n",
    "train_y3 = part_train_y3\n",
    "train_y = part_train_y1\n",
    "params = {\n",
    "    'booster': 'gbtree',\n",
    "    'min_child_weight': 10.0,\n",
    "    'learning_rate': 0.02,\n",
    "    'objective': 'reg:squarederror',\n",
    "    # 'eval_metric': \"mape\",\n",
    "    'max_depth': 7,\n",
    "    'max_delta_step': 1.8,\n",
    "    'colsample_bytree': 0.4,\n",
    "    'subsample': 0.8,\n",
    "    'eta': 0.025,\n",
    "    'gamma': 0.65,\n",
    "    'nthread': -1,\n",
    "    'seed': 111,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "folds = KFold(n_splits=2, shuffle=True, random_state=111)\n",
    "score = 0.0\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_x)):\n",
    "    print(\"fold {}\".format(fold_ + 1))\n",
    "    trn_data = xgb.DMatrix(train_x.iloc[trn_idx], label=train_y[trn_idx])\n",
    "    val_data = xgb.DMatrix(train_x.iloc[val_idx], label=train_y[val_idx])\n",
    "\n",
    "    watchlist = [(trn_data, 'train'), (val_data, 'valid')]\n",
    "    clf = xgb.train(params,\n",
    "                    trn_data,\n",
    "                    num_boost_round=1000,\n",
    "                    evals=watchlist,\n",
    "                    custom_metric=mape_evaluate,\n",
    "                    verbose_eval=200,)\n",
    "\n",
    "    print(clf.best_iteration)\n",
    "print(score)\n",
    "    # test_pred_prob += clf.predict(xgb.DMatrix(test), ntree_limit=clf.best_iteration) / folds.n_splits"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}